[["index.html", "R cookbook for the casual dabbler (2nd edition) Chapter 1 Introduction 1.1 Usage 1.2 Additional resources 1.3 Limitations 1.4 About the author", " R cookbook for the casual dabbler (2nd edition) Charles Coverdale 2025-02-15 Chapter 1 Introduction G’day and welcome to R cookbook for the casual dabbler (2nd edition). RCCD 1st edition was originally published in 2020 as a side project during the COVID-19 pandemic in Melbourne. As I wrote in the midst of lockdown: I use R a lot for work and for side projects. Over the years I’ve collated a bunch of useful scripts, from macroeconomic analysis to quick hacks for making map legends format properly. Historically my code has been stored in random Rpubs documents, medium articles, and a bunch of .Rmd files on my hardrive. Occasionally I feel like doing things properly - and upload code to a repository on github. It doesn’t take a genius to realize this isn’t a very sustainable solution - and it also isn’t very useful for sharing code with others. It turns out 2-years of lockdown in Melbourne was enough incentive to sit down and collate my best and most useful code into a single place. In the spirit of open source, a book seemed like the most logical format. The following is a very rough book written in markdown - R’s very own publishing language. RCCD1e had surprisingly good (and long-lasting reviews). Alas, five-years on, a lot has changed - in both the R community, and the world more broadly. As such, RCCD2e includes significant revisions. The most major of these is a restructure to make the chapters flow more logically. The Australian specific chapters have also been grouped together (e.g. election data and ABS economic indicators). 1.1 Usage In each chapter I’ve written up the background, methodology and code for a separate piece of analysis. Most of this code will not be extraordinary to the seasoned R aficionado. However I find that in classic Pareto style ~20% of my code contributes to the vast majority of my work output. Having this 20% on hand will hopefully be useful to both myself and others. 1.2 Additional resources The R community is continually writing new packages and tools. Many of these are covered extensively in various free books available on the bookdown.org website. The rise of LLM’s over the past 2-years has also made it significantly easier to find, refine, and expand on R code. RCCD2e includes optimized (and better formated code) which has gone through the scrutiny of many of the different LLM’s. I encourage users to paste in code snippets to language models for deeper explanations or alternate examples. 1.3 Limitations If you find a bug (along with spelling errors etc) please email me at charlesfcoverdale@gmail.com with the subject line ‘RCCD2e’. 1.4 About the author Charles Coverdale is an economist working across London and Melbourne. He is passionate about economics, climate science, and building talented teams. You can get in touch with Charles on twitter to hear more about his current projects. "],["making-maps-beautiful.html", "Chapter 2 Making maps beautiful 2.1 Why use a map 2.2 Getting started 2.3 From okay to good 2.4 From good to great 2.5 From great to fantastic", " Chapter 2 Making maps beautiful 2.1 Why use a map Maps are a great way to communicate data. They’re easily understandable, flexible, and more intuitive than a chart. There’s been numerous studies showing that the average reader often struggles to interpret the units on a y-axis, let alone understand trends in scatter or line graphs. Making maps in R takes some initial investment. However once you have some code you know and understand, spinning up new pieces of analysis can happen in minutes, rather than hours or days. The aim of this chapter is to get you from ‘I can make a map in R’ to something more like ‘I can conduct spatial analysis and produce a visual which is ready for publication’. 2.2 Getting started First up, we need to load a bunch of mapping packages. The tidyverse package is a classic for just about everything data manipulation, while, the sf and ggspatial packages are essential for making maps. # Load required packages library(tidyverse) # Includes ggplot2, dplyr, tidyr, readxl, purrr library(ggmap) library(sf) library(ggspatial) library(rlang) library(broom) library(Census2016) library(strayr) library(absmapsdata) library(officer) Will Mackey’s absmapsdata package contains all the ABS’ ASGS shapefiles. The data is now also callable through the (thoroughly updated and expanded) strayr package. For example: strayr::read_absmap(&quot;sa12016&quot;) To get a basic demographic map up and running, we will splice together the shapefile (in this case the SA2 map of Austrlaia) and some data from the 2016 Australian Census. Hugh Parsonage put together a fantastic packaged called Census2016 which makes downloading this data in a clean format easy. #Get the shapefile form the absmapsdata package (predefined in the list above) #Get the 2016 census dataset census2016_wide &lt;- Census2016_wide_by_SA2_year #Select the key demographic columns from the census data (i.e. the first 8 variables) census_short &lt;- census2016_wide[,1:8] #Filter for a single year census_short_2016 &lt;- census_short %&gt;% filter(year==2016) #Use the inner_join function to get the shapefile and census wide data into a single df for analysis / visualisation SA2_shp_census_2016 &lt;- inner_join(strayr::read_absmap(&quot;sa22016&quot;),census_short_2016, by = c(&quot;sa2_name_2016&quot; = &quot;sa2_name&quot;)) #Plot a map that uses census data map1 &lt;- ggplot() + geom_sf(data = SA2_shp_census_2016, aes(fill = median_age)) + ggtitle(&quot;Australian median age (SA2)&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) map1 There we go! A map. This looks ‘okay’… but it can be much better. 2.3 From okay to good Heat maps don’t really show too much interesting data on such a large scale, so let’s filter down to Greater Melbourne. Seeing we have a bunch of census data in our dataframe, we can also do some basic analysis (e.g. population density). #As a bit of an added extra, we can create a new population density column SA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;% mutate(pop_density=persons/areasqkm_2016) #Filter for Greater Melbourne MEL_SA2_shp_census_2016 &lt;- SA2_shp_census_2016 %&gt;% filter(gcc_name_2016==&quot;Greater Melbourne&quot;) #Plot the new map just for Greater Melbourne map2 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age, border=NA)) + ggtitle(&quot;Median age in Melbourne (SA2)&quot;) + xlab(&quot;Longitude&quot;) + ylab(&quot;Latitude&quot;) + theme_bw() + theme(legend.position = &quot;right&quot;) map2 Much better. We can start to see some trends in this map. It looks like younger people tend to live closer to the city center. This seems logical. 2.4 From good to great The map above is a good start! However, how do we turn this from something ‘good’, into something that is 100% ready to share? We see our ‘ink to chart ratio’ (i.e. the amount of non-data stuff that is on the page) is still pretty high. Is the latitude of Melbourne useful for this analysis…? Not really. Let’s get rid of it and the axis labels. A few lines of code adjusting the axis, titles, and theme of the plot will go a long way. Because my geography Professor drilled it into me, I will also add a low-key scale bar. map3 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age)) + labs(title=&quot;Melbourne&#39;s youth tend to live closer to the city centre&quot;, subtitle = &quot;Analysis from the 2016 census&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;Median age&quot;) + ggspatial::annotation_scale(location=&quot;br&quot;)+ theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) map3 2.5 From great to fantastic The above is perfectly reasonable and looks professionally designed. However, this is where we can get really special. Let’s add a custom colour scheme, drop the boundary edges for the SA2’s, and add in a dot and label for Melbourne CBD. #Add in a point for the Melbourne CBD MEL_location &lt;- data.frame(town_name = c(&quot;Melbourne&quot;), x = c(144.9631), y = c(-37.8136)) map4 &lt;- ggplot() + geom_sf(data = MEL_SA2_shp_census_2016, aes(fill = median_age),color=NA) + geom_point(data=MEL_location,aes(x=x,y=y),size=2,color=&quot;black&quot;)+ labs(title=&quot;Melbourne&#39;s youth tend to live closer to the city centre&quot;, subtitle = &quot;Analysis from the 2016 census&quot;, caption = &quot;Data: Australian Bureau of Statistics 2016&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;Median age&quot;) + scale_fill_steps(low=&quot;#E2E0EB&quot;, high=&quot;#3C33FE&quot;)+ annotate(geom=&#39;curve&#39;, x=144.9631, y=-37.8136, xend=144.9, yend=-38.05, curvature=0.5, arrow=arrow(length=unit(2,&quot;mm&quot;)))+ annotate(geom=&#39;text&#39;,x=144.76,y=-38.1,label=&quot;Melbourne CBD&quot;)+ ggspatial::annotation_scale(location=&quot;br&quot;)+ theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;right&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) map4 Now we’re talking. A ‘client-ready’ looking map that can be added to a report, presentation, or with a few tweaks - a digital dashboard. Make sure to export the map as a high quality PNG using the ggplot2:ggsave() function. "],["charts.html", "Chapter 3 Charts 3.1 Packages matter 3.2 Make the data tidy 3.3 Line plot 3.4 Scatter and trend plot 3.5 Shading areas on plots 3.6 Bar chart 3.7 Stacked bar chart 3.8 Histogram 3.9 Ridge chart 3.10 BBC style: Bar charts (categorical) 3.11 BBC style: Dumbbell charts 3.12 Facet wraps 3.13 Pie chart 3.14 Patchwork 3.15 Saving to powerpoint 3.16 Automating chart creation", " Chapter 3 Charts 3.1 Packages matter There’s exceptional resources online for using the ggplot2 package and the broader tidyverse suite to create production ready charts. The R Graph Gallery is a great place to start, as is the visual storytelling blogs of The Economist and the BBC. This chapter contains the code for some of my most used charts and visualization techniques. # Load core packages library(tidyverse) # Includes ggplot2, dplyr, tidyr, purrr, and readr library(lubridate) library(readxl) library(scales) # Load additional visualization libraries library(ggridges) library(ggrepel) library(viridis) library(patchwork) # Load additional data manipulation libraries library(reshape2) library(gapminder) 3.2 Make the data tidy Before making a chart ensure the data is “tidy” - meaning there is a new row for every changed variable. It also doesn’t hurt to remove NA’s for consistency (particularly in time series). #Read in data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; #Read in with read.xlsx MEL_temp_daily &lt;- openxlsx::read.xlsx(url) #Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) #Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) #Add in a year MEL_temp_daily[&quot;Year&quot;]=2019 #Reorder MEL_temp_daily &lt;- MEL_temp_daily[,c(1,2,4,3)] #Make a single data field using lubridate MEL_temp_daily &lt;- MEL_temp_daily %&gt;% mutate(Date = make_date(Year, Month, Day)) #Drop the original date columns MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::select(Date, Temp) %&gt;% drop_na() #Add on a 7-day rolling average MEL_temp_daily &lt;- MEL_temp_daily %&gt;% dplyr::mutate(Seven_day_rolling = zoo::rollmean(Temp, k = 7, fill = NA), Mean = mean(Temp)) #Drop NA&#39;s #MEL_temp_daily &lt;- MEL_temp_daily %&gt;% drop_na() 3.3 Line plot plot_MEL_temp &lt;- ggplot(MEL_temp_daily, aes(x = Date)) + geom_line(aes(y = Temp), col = &quot;blue&quot;) + geom_line(aes(y = Mean), col = &quot;orange&quot;) + labs( title = &quot;Hot in the summer and cool in the winter&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meteorology 2019&quot;, x = &quot;&quot;, y = &quot;&quot; ) + scale_x_date( date_breaks = &quot;1 month&quot;, date_labels = &quot;%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;, &#39;2019-12-14&#39;)) ) + scale_y_continuous(labels = unit_format(unit = &quot;\\u00b0C&quot;, sep = &quot;&quot;)) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) + annotate( geom = &quot;curve&quot;, x = as.Date(&#39;2019-08-01&#39;), y = 23, xend = as.Date(&#39;2019-08-01&#39;), yend = 17, curvature = -0.5, arrow = arrow(length = unit(2, &quot;mm&quot;)) ) + annotate( geom = &quot;text&quot;, x = as.Date(&#39;2019-07-15&#39;), y = 25, label = &quot;Below 20°C all winter&quot; ) plot_MEL_temp 3.4 Scatter and trend plot MEL_temp_Jan &lt;- MEL_temp_daily %&gt;% filter(Date &lt; as.Date(&quot;2019-01-31&quot;)) ggplot(MEL_temp_Jan, aes(x = Date, y = Temp)) + geom_point(col = &quot;purple&quot;, alpha = 0.4) + geom_smooth(col = &quot;purple&quot;, fill = &quot;purple&quot;, alpha = 0.1, method = &quot;lm&quot;) + labs( title = &quot;January is a hot one&quot;, subtitle = &quot;Analysing temperature in Melbourne&quot;, caption = &quot;Data: Bureau of Meteorology 2019&quot;, x = &quot;&quot;, y = &quot;Temperature °C&quot; ) + scale_x_date( date_breaks = &quot;1 week&quot;, date_labels = &quot;%d-%b&quot;, limits = as.Date(c(&#39;2019-01-01&#39;, &#39;2019-01-31&#39;)), expand = c(0, 0) ) + geom_hline(yintercept = 45, colour = &quot;black&quot;, size = 0.4) + annotate( geom = &quot;curve&quot;, x = as.Date(&#39;2019-01-22&#39;), y = 37.5, xend = as.Date(&#39;2019-01-25&#39;), yend = 42, curvature = 0.5, col = &quot;#575757&quot;, arrow = arrow(length = unit(2, &quot;mm&quot;)) ) + annotate( geom = &quot;text&quot;, x = as.Date(&#39;2019-01-16&#39;), y = 37.5, label = &quot;January saw some extreme temperatures&quot;, size = 3.2, col = &quot;#575757&quot; ) + theme_minimal() + theme( plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), axis.title.y = element_text(size = 9, margin = margin(r = 10)), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) 3.5 Shading areas on plots Adding shading behind a plot area is simple using geom_rect. Adding shading under a particular model line? A little trickier. See both example below. # Example 1: Custom y-axis threshold shading threshold &lt;- 20 ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point() + geom_hline(yintercept = threshold) + # Shade areas below and above threshold geom_rect(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = threshold, fill = &quot;blue&quot;, alpha = 0.2) + geom_rect(xmin = -Inf, xmax = Inf, ymin = threshold, ymax = Inf, fill = &quot;red&quot;, alpha = 0.2) # Example 2: Model line with shaded areas model &lt;- lm(mpg ~ log(hp), data = mtcars) # Generate predicted values for plotting df_line &lt;- data.frame(hp = seq(min(mtcars$hp), max(mtcars$hp), by = 1)) df_line$mpg &lt;- predict(model, newdata = df_line) # Define polygons above and below the model line df_poly_under &lt;- bind_rows(df_line, tibble(hp = c(max(df_line$hp), min(df_line$hp)), mpg = c(-Inf, -Inf))) df_poly_above &lt;- bind_rows(df_line, tibble(hp = c(max(df_line$hp), min(df_line$hp)), mpg = c(Inf, Inf))) # Plot the data, model line, and shaded areas ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point(color = &quot;grey&quot;, alpha = 0.5) + geom_line(data = df_line, aes(x = hp, y = mpg), color = &quot;black&quot;) + # Shaded areas geom_polygon(data = df_poly_under, aes(x = hp, y = mpg), fill = &quot;blue&quot;, alpha = 0.2) + geom_polygon(data = df_poly_above, aes(x = hp, y = mpg), fill = &quot;red&quot;, alpha = 0.2) + scale_x_continuous(expand = c(0, 0)) + labs( title = &quot;Look at that snazzy red/blue shaded area&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme_minimal() + theme( panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) 3.6 Bar chart # Create data frame directly bar_data_single &lt;- data.frame( Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;), Value = c(1000000, 3000000, 2000000, 5000000) ) ggplot(bar_data_single, aes(x = Year, y = Value)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;blue&quot;, width = 0.8) + # Labels in the middle of the bars geom_text(aes(y = Value / 2, label = scales::dollar(Value, scale = 1/1e6, suffix = &quot;m&quot;)), size = 5, color = &quot;white&quot;, fontface = &quot;bold&quot;) + labs( title = &quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme_minimal() + theme( plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 12), axis.text = element_text(size = 12), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) # Save the plot # ggsave(&quot;test.png&quot;, width = 10, height = 10, units = &quot;cm&quot;, dpi = 600) 3.7 Stacked bar chart Year = c(&quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;, &quot;2020&quot;) Quarter = c(&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;,&quot;Q1&quot;,&quot;Q2&quot;,&quot;Q3&quot;, &quot;Q4&quot;) Value = (c(100,300,200,500,400,700,200,300)) bar_data &lt;- (cbind(Year, Quarter, Value)) bar_data &lt;- as.data.frame(bar_data) bar_data$Value = as.integer(bar_data$Value) bar_data_totals &lt;- bar_data %&gt;% dplyr::group_by(Year) %&gt;% dplyr:: summarise(Total = sum(Value)) ggplot(bar_data, aes(x = Year, y = Value, fill = (Quarter), label=Value)) + geom_bar(position = position_stack(reverse=TRUE),stat=&#39;identity&#39;)+ geom_text(size = 4, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(reverse=TRUE,vjust = 0.5), label=scales::dollar(Value))+ geom_text(aes(Year, Total, label=scales::dollar(Total), fill = NULL, vjust=-0.5), fontface=&quot;bold&quot;, size=4, data = bar_data_totals)+ scale_fill_brewer(palette = &quot;Blues&quot;) + labs(title=&quot;Bar chart example&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;Units&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=10))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + scale_y_continuous(expand=c(0,0),limits=c(0,1800))+ theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) 3.8 Histogram Aka. a bar chart for a continuous variable where the bars are touching. Useful to show distribution of time series or ordinal variables. c(&quot;0-20&quot;, &quot;20-40&quot;, &quot;40-60&quot;, &quot;60-80&quot;, &quot;80+&quot;) ## [1] &quot;0-20&quot; &quot;20-40&quot; &quot;40-60&quot; &quot;60-80&quot; &quot;80+&quot; #Create a data set hist_data &lt;- data.frame(X1=sample(0:100,100,rep=TRUE)) ggplot(hist_data)+ geom_histogram(aes(x=X1),binwidth=5,fill=&quot;blue&quot;,alpha=0.5) + geom_vline(xintercept=c(50,75,95),yintercept=0,linetype=&quot;longdash&quot;,col=&quot;orange&quot;) + labs(title=&quot;Histogram example&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 3.9 Ridge chart Handy when working with climate variables. Particularly useful at showing the difference in range of multiples series (e.g. temperature by month). # Import data url &lt;-&quot;https://raw.githubusercontent.com/charlescoverdale/ggridges/master/2019_MEL_max_temp_daily.xlsx&quot; MEL_temp_daily &lt;- openxlsx::read.xlsx(url) # Remove last 2 characters to just be left with the day number MEL_temp_daily$Day=substr(MEL_temp_daily$Day,1,nchar(MEL_temp_daily$Day)-2) # Make a wide format long using the gather function MEL_temp_daily &lt;- MEL_temp_daily %&gt;% gather(Month,Temp,Jan:Dec) MEL_temp_daily$Month&lt;-factor(MEL_temp_daily$Month,levels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;)) # Plot ggplot(MEL_temp_daily, aes(x = Temp, y = Month, fill = stat(x))) + geom_density_ridges_gradient(scale =2, size=0.3, rel_min_height = 0.01, gradient_lwd = 1.) + scale_y_discrete(limits = unique(rev(MEL_temp_daily$Month)))+ scale_fill_viridis_c(name = &quot;°C&quot;, option = &quot;C&quot;) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot; &quot;)+ ylab(&quot; &quot;)+ theme_ridges(font_size = 13, grid = TRUE) 3.10 BBC style: Bar charts (categorical) #devtools::install_github(&#39;bbc/bbplot&#39;) library(gapminder) library(bbplot) # Prepare data bar_df &lt;- gapminder %&gt;% filter(year == 2007 &amp; continent == &quot;Africa&quot;) %&gt;% arrange(desc(lifeExp)) %&gt;% head(5) # Make plot bars &lt;- ggplot(bar_df, aes(x = reorder(country, lifeExp), y = lifeExp, fill = country == &quot;Mauritius&quot;)) + geom_bar(stat=&quot;identity&quot;, position=&quot;identity&quot;) + geom_hline(yintercept = 0, size = 1, colour=&quot;#333333&quot;) + scale_fill_manual(values = c(&quot;TRUE&quot; = &quot;#1380A1&quot;, &quot;FALSE&quot; = &quot;#dddddd&quot;)) + labs(title = &quot;Mauritius has the highest life expectancy&quot;, subtitle = &quot;Top 5 African countries by life expectancy, 2007&quot;) + coord_flip() + theme_minimal() + theme(panel.grid.major.x = element_line(color=&quot;#cbcbcb&quot;), panel.grid.major.y = element_blank(), legend.position = &quot;none&quot;) # Add labels labelled_bars &lt;- bars + geom_label(aes(label = round(lifeExp, 0)), hjust = 1, vjust = 0.5, colour = &quot;white&quot;, fill = &quot;black&quot;, label.size = 0.2, family = &quot;Helvetica&quot;, size = 6) labelled_bars 3.11 BBC style: Dumbbell charts Dumbbell charts are handy instead of using clustered column charts with janky thinkcell labels and arrows to show the difference between the columns. Note it relies on having a 4 variable input (variable_name, value1, value2, and gap). The geom_dumbellfunction lives inside the ggalt package rather than the standard ggplot2. library(ggalt) # For geom_dumbbell library(gapminder) library(bbplot) # Uncomment if using bbc_style() # Prepare data dumbbell_df &lt;- gapminder %&gt;% filter(year %in% c(1967, 2007)) %&gt;% select(country, year, lifeExp) %&gt;% pivot_wider(names_from = year, values_from = lifeExp) %&gt;% mutate(gap = `2007` - `1967`) %&gt;% arrange(desc(gap)) %&gt;% head(10) # Make plot ggplot(dumbbell_df, aes(x = `1967`, xend = `2007`, y = reorder(country, gap), group = country)) + geom_dumbbell(colour = &quot;#dddddd&quot;, size = 3, colour_x = &quot;#FAAB18&quot;, colour_xend = &quot;#1380A1&quot;) + labs(title = &quot;We&#39;re Living Longer&quot;, subtitle = &quot;Top 10 biggest increases in life expectancy (1967-2007)&quot;) + theme_minimal() + theme(panel.grid.major.y = element_blank(), panel.grid.minor = element_blank()) 3.12 Facet wraps Handy rather than showing multiple lines on the same chart. Top tips: facet_wrap()dataframes need to be in long form in order to be manipulated easily. It also helps to add on separate columns for the start and end values (if you want to add data point labels). #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) QLD = (c(500,300, 500, 600)) NSW = (c(200,400, 500, 700)) VIC = (c(300,400, 500, 600)) #Combine the columns into a single dataframe facet_data &lt;- (cbind(Year, QLD,NSW,VIC)) facet_data &lt;- as.data.frame(facet_data) #Change formats to integers facet_data$QLD = as.integer(facet_data$QLD) facet_data$NSW = as.integer(facet_data$NSW) facet_data$VIC = as.integer(facet_data$VIC) #Make the wide data long facet_data_long &lt;- pivot_longer(facet_data,!Year, names_to=&quot;State&quot;, values_to=&quot;Value&quot;) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(start_label = if_else(Year == min(Year), as.integer(Value), NA_integer_)) facet_data_long &lt;- facet_data_long %&gt;% dplyr::mutate(end_label = if_else(Year == max(Year), as.integer(Value), NA_integer_)) #Make the base line chart base_chart &lt;- ggplot() + geom_line(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + geom_point(data=facet_data_long, aes(x = Year, y = Value, group = State, colour = State)) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = end_label), color = &quot;black&quot;, nudge_y = -10,size=3) + ggrepel::geom_text_repel(data=facet_data_long, aes(x = Year, y = Value, label = start_label), color = &quot;black&quot;, nudge_y = 10,size=3) base_chart + scale_x_discrete( breaks = seq(2018, 2021, 1), labels = c(&quot;2018&quot;, &quot;19&quot;, &quot;20&quot;, &quot;21&quot;))+ facet_wrap(State ~ .) + #To control the grid arrangement, we can add in customer dimensions #ncol = 2, nrow=2) + labs(title=&quot;State by state comparison&quot;, subtitle = &quot;Facet wraps are looking good&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(strip.text.x = element_text(size = 9, face = &quot;bold&quot;)) + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) 3.13 Pie chart These should be used sparingly… but they are handy for showing proportions when the proportion of the whole is paramount (e.g. 45%) - rather than the proportion in relation to another data point (e.g. 16% one year vs 18% the next). # Create Data pie_data &lt;- data.frame( group=LETTERS[1:5], value=c(13,7,9,21,2)) # Compute the position of labels pie_data &lt;- pie_data %&gt;% arrange(desc(group)) %&gt;% mutate(proportion = value / sum(pie_data$value) *100) %&gt;% mutate(ypos = cumsum(proportion)- 0.5*proportion ) # Basic piechart ggplot(pie_data, aes(x=&quot;&quot;, y=proportion, fill=group)) + geom_bar(stat=&quot;identity&quot;)+ coord_polar(&quot;y&quot;, start=0) + theme_void()+ geom_text(aes(y = ypos, label = paste(round(proportion,digits=0),&quot;%&quot;, sep = &quot;&quot;),x=1.25), color = &quot;white&quot;, size=4) + scale_fill_brewer(palette=&quot;Set1&quot;)+ labs(title=&quot;Use pie charts sparingly&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme(legend.position = &quot;bottom&quot;)+ theme(legend.title = element_blank())+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(plot.subtitle = element_text(margin=margin(0,0,5,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) # ggsave(plot=last_plot(), # width=10, # height=10, # units=&quot;cm&quot;, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/pietest.png&quot;) 3.14 Patchwork Patchwork is a nifty package for arranging plots and other graphic elements (text, tables etc) in different grid arrangements. The basic syntax is to use plot1 | plot2 for side by side charts, and plot1 / plot2for top and bottom charts. You can also combine these two functions for a grid of different size columns (e.g. plot3 / (plot1 | plot2) #Make some simply plots using the mtcars package p1 &lt;- ggplot(mtcars) + geom_point(aes(mpg, disp)) + ggtitle(&#39;Plot 1&#39;) p2 &lt;- ggplot(mtcars) + geom_boxplot(aes(gear, disp, group = gear)) + ggtitle(&#39;Plot 2&#39;) #Example of side by side charts library(patchwork) p1 + p2 #Add in a table next to the plot p1 + gridExtra::tableGrob(mtcars[1:10, c(&#39;mpg&#39;, &#39;disp&#39;)]) 3.15 Saving to powerpoint There’s a bunch of ways to save ggplot graphics - but the way I find most useful is by exporting to pptx in a common ‘charts’ directory. If you want to save as a png you can use the normal ggsave function - however it will not be editable (e.g. able to click and drag to rescale for a presentation). Therefore instead we can use the grattantheme package to easily save to an editable pptx graphic. Note: The code below has been commented out so that is will upload to bookdown.org without an error. #The classic save function to png # ggsave(plot = ggplot2::last_plot(), # width = 8, # height = 12, # dpi = 600, # filename = &quot;/Users/charlescoverdale/Desktop/test.png&quot;) #Using the grattantheme package to easily safe to powerpoint # grattan_save_pptx(p = ggplot2::last_plot(), # &quot;/Users/charlescoverdale/Desktop/test.pptx&quot;, # type = &quot;wholecolumn&quot;) 3.16 Automating chart creation Let’s say we have a dataframe of multiple variables. We want to produce simple charts of the same style for each variable (including formatting and titles etc). Sure we can change the aes(x=)and aes(y=) variables in ggplot2 manually for each column - but this is time intensive especially for large data frames. Instead, we can write a function that will loop through the whole dataframe and produce the same format of chart. #Create a data set Year = c(&quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;) Variable1 = (c(500,300, 200, 400)) Variable2 = (c(200,400, 200, 700)) Variable3 = (c(300,500, 800, 1000)) #Combine the columns into a single dataframe bar_data_multiple &lt;- (cbind(Year, Variable1, Variable2, Variable3)) bar_data_multiple &lt;- as.data.frame(bar_data_multiple) #Change formats to integers bar_data_multiple$Variable1 = as.integer(bar_data_multiple$Variable1) bar_data_multiple$Variable2 = as.integer(bar_data_multiple$Variable2) bar_data_multiple$Variable3 = as.integer(bar_data_multiple$Variable3) #Define a function loop &lt;- function(chart_variable) { ggplot(bar_data_multiple, aes(x = Year, y = .data[[chart_variable]], label = .data[[chart_variable]]))+ geom_bar(stat=&#39;identity&#39;,fill=&quot;blue&quot;)+ geom_text(aes( label = scales::dollar(.data[[chart_variable]])), size = 5, col=&quot;white&quot;, fontface=&quot;bold&quot;, position = position_stack(vjust = 0.5))+ labs(title=paste(&quot;Company X: &quot;, chart_variable, &quot; (&quot;,head(Year,n=1), &quot; - &quot;, tail(Year,n=1), &quot;)&quot;, sep=&quot;&quot;), subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Made up from scratch&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=12))+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank())+ theme(panel.grid.major.y = element_blank()) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank()) } plots &lt;- purrr::map(colnames(bar_data_multiple)[colnames(bar_data_multiple) != &quot;Year&quot;], loop) plots #cowplot::plot_grid(plotlist = plots) "],["basic-modelling.html", "Chapter 4 Basic modelling 4.1 Source, format, and plot data 4.2 Build a linear model 4.3 Analyse the model fit 4.4 Compare the predicted values with the actual values 4.5 Analyse the residuals 4.6 Linear regression with more than one variable 4.7 Fitting a polynomial regression", " Chapter 4 Basic modelling Creating a model is an essential part of forecasting and data analysis. I’ve put together a quick guide on my process for modelling data and checking model fit. The source data I use in this example is Melbourne’s weather record over a 12 month period. Daily temperature is based on macroscale weather and climate systems, however many observable measurements are correlated (i.e. hot days tend to have lots of sunshine). This makes using weather data great for model building. 4.1 Source, format, and plot data Before we get started, it is useful to have some packages up and running. #Useful packages for regression library(readr) library(readxl) library(ggplot2) library(tidyverse) library(lubridate) library(modelr) library(cowplot) I’ve put together a csv file of weather observations in Melbourne in 2019. We begin our model by downloading the data from Github. #Input data link &lt;- &quot;data/MEL_weather_2019.csv&quot; # We&#39;ll read this data in as a dataframe # The &#39;check.names&#39; function set to false means the funny units that the BOM use for column names won&#39;t affect the import. MEL_weather_2019 &lt;- read.csv(link, check.names = F) head(MEL_weather_2019) This data is relatively clean. One handy change to make is to make the date into a dynamic format (to easily switch between months, years, etc). #Add a proper date column MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Date = make_date(Year, Month, Day)) We also notice that some of the column names have symbols in them. This can be tricky to work with, so let’s rename some columns into something more manageable. #Rename key df variables names(MEL_weather_2019)[4]&lt;- &quot;Solar_exposure&quot; names(MEL_weather_2019)[5]&lt;- &quot;Rainfall&quot; names(MEL_weather_2019)[6]&lt;- &quot;Max_temp&quot; head(MEL_weather_2019) We’re aiming to investigate if other weather variables can predict maximum temperatures. Solar exposure seems like a plausible place to start. We start by plotting the two variables to if there is a trend. #Plot the data MEL_temp_investigate &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure),col=&quot;grey&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_investigate Eyeballing the chart above, there seems to be a correlation between the two data sets. We’ll do one more quick plot to analyse the data. What is the distribution of temperature? ggplot(MEL_weather_2019, aes(x=Max_temp)) + geom_histogram(aes(y=..density..), colour=&quot;black&quot;, fill=&quot;lightblue&quot;)+ geom_density(alpha=.5, fill=&quot;grey&quot;,colour=&quot;darkblue&quot;)+ scale_x_continuous(breaks=c(5,10,15,20,25,30,35,40,45), expand=c(0,0))+ xlab(&quot;Temperature&quot;)+ ylab(&quot;Density&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) We can see here the data is right skewed (i.e. the mean will be greater than the median). We’ll need to keep this in mind. Let’s start building a model. 4.2 Build a linear model We start by looking whether a simple linear regression of solar exposure seems to be correlated with temperature. In R, we can use the linear model (lm) function. #Create a straight line estimate to fit the data temp_model &lt;- lm(Max_temp~Solar_exposure, data=MEL_weather_2019) 4.3 Analyse the model fit Let’s see how well solar exposure explains changes in temperature #Call a summary of the model summary(temp_model) The adjusted R squared value (one measure of model fit) is 0.3596. Furthermore the coefficient of our solar_exposure variable is statistically significant. 4.4 Compare the predicted values with the actual values We can use this lm function to predict values of temperature based on the level of solar exposure. We can then compare this to the actual temperature record, and see how well the model fits the data set. #Use this lm model to predict the values MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(predicted_temp=predict(temp_model,newdata=MEL_weather_2019)) #Calculate the prediction interval prediction_interval &lt;- predict(temp_model, newdata=MEL_weather_2019, interval = &quot;prediction&quot;) summary(prediction_interval) #Bind this prediction interval data back to the main set MEL_weather_2019 &lt;- cbind(MEL_weather_2019,prediction_interval) MEL_weather_2019 Model fit is easier to interpret graphically. Let’s plot the data with the model overlaid. #Plot a chart with data and model on it MEL_temp_predicted &lt;- ggplot(MEL_weather_2019)+ geom_point(aes(y=Max_temp, x=Solar_exposure), col=&quot;grey&quot;)+ geom_line(aes(y=predicted_temp,x=Solar_exposure), col=&quot;blue&quot;)+ geom_smooth(aes(y=Max_temp, x= Solar_exposure), method=lm)+ geom_line(aes(y=lwr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ geom_line(aes(y=upr,x=Solar_exposure), colour=&quot;red&quot;, linetype=&quot;dashed&quot;)+ labs(title = &quot;Does solar exposure drive temperature in Melbourne?&quot;, subtitle = &#39;Investigation using linear regression&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature °C&quot;)+ scale_x_continuous(expand=c(0,0), breaks=c(0,5,10,15,20,25,30,35,40))+ theme_bw()+ theme(axis.text=element_text(size=10))+ theme(panel.grid.minor = element_blank()) MEL_temp_predicted This chart includes the model (blue line), confidence interval (grey band around the blue line), and a prediction interval (red dotted line). A prediction interval reflects the uncertainty around a single value (put simple: what is the reasonable upper and lower bound that this data point could be estimated at?). A confidence interval reflects the uncertainty around the mean prediction values (put simply: what is a reasonable upper and lower bound for the blue line at this x value?). Therefore, a prediction interval will be generally much wider than a confidence interval for the same value. 4.5 Analyse the residuals #Add the residuals to the series residuals_temp_predict &lt;- MEL_weather_2019 %&gt;% add_residuals(temp_model) Plot these residuals in a chart. residuals_temp_predict_chart &lt;- ggplot(data=residuals_temp_predict, aes(x=Solar_exposure, y=resid), col=&quot;grey&quot;)+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_point(col=&quot;grey&quot;)+ xlab(&quot;Solar exposure&quot;)+ ylab(&quot;Maximum temperature (°C)&quot;)+ theme_bw() + labs(title = &quot;Residual values from the linear model&quot;)+ theme(axis.text=element_text(size=12))+ scale_x_continuous(expand=c(0,0)) residuals_temp_predict_chart 4.6 Linear regression with more than one variable The linear model above is *okay*, but can we make it better? Let’s start by adding in some more variables into the linear regression. Rainfall data might assist our model in predicting temperature. Let’s add in that variable and analyse the results. temp_model_2 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall, data=MEL_weather_2019) summary(temp_model_2) We can see that adding in rainfall made the model better (R squared value has increased to 0.4338). Next, we consider whether solar exposure and rainfall might be related to each other, as well as to temperature. For our third temperature model, we add an interaction variable between solar exposure and rainfall. temp_model_3 &lt;- lm(Max_temp ~ Solar_exposure + Rainfall + Solar_exposure:Rainfall, data=MEL_weather_2019) summary(temp_model_3) We now see this variable is significant, and improves the model slightly (seen by an adjusted R squared of 0.4529). 4.7 Fitting a polynomial regression When analysing the above data set, we see the issue is the sheer variance of temperatures associated with every other variable (it turns out weather forecasting is notoriously difficult). However we can expect that temperature follows a non-linear pattern throughout the year (in Australia it is hot in January-March, cold in June-August, then starts to warm up again). A linear model (e.g. a straight line) will be a very bad model for temperature — we need to introduce polynomials. For simplicity, we will introduce a new variable (Day_number) which is the day of the year (e.g. 1 January is #1, 31 December is #366). MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(Day_number=row_number()) head(MEL_weather_2019) Using the same dataset as above, let’s plot temperature in Melbourne in 2019. MEL_temp_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x = Day_number, y = Max_temp)) + labs(title = &#39;Melbourne temperature profile&#39;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlab(&quot;Day of the year&quot;)+ ylab(&quot;Temperature&quot;)+ theme_bw() MEL_temp_chart We can see we’ll need a non-linear model to fit this data. Below we create a few different models. We start with a normal straight line model, then add an x² and x³ model. We then use these models and the ‘predict’ function to see what temperatures they forecast based on the input data. #Create a straight line estimate to fit the data poly1 &lt;- lm(Max_temp ~ poly(Day_number,1,raw=TRUE), data=MEL_weather_2019) summary(poly1) #Create a polynominal of order 2 to fit this data poly2 &lt;- lm(Max_temp ~ poly(Day_number,2,raw=TRUE), data=MEL_weather_2019) summary(poly2) #Create a polynominal of order 3 to fit this data poly3 &lt;- lm(Max_temp ~ poly(Day_number,3,raw=TRUE), data=MEL_weather_2019) summary(poly3) #Use these models to predict MEL_weather_2019 &lt;- MEL_weather_2019 %&gt;% mutate(poly1values=predict(poly1,newdata=MEL_weather_2019))%&gt;% mutate(poly2values=predict(poly2,newdata=MEL_weather_2019))%&gt;% mutate(poly3values=predict(poly3,newdata=MEL_weather_2019)) head(MEL_weather_2019) In the table above we can see the estimates for that data point from the various models. To see how well the models did graphically, we can plot the original data series with the polynominal models overlaid. #Plot a chart with all models on it MEL_weather_model_chart &lt;- ggplot(MEL_weather_2019)+ geom_line(aes(x=Day_number, y= Max_temp),col=&quot;grey&quot;)+ geom_line(aes(x=Day_number, y= poly1values),col=&quot;red&quot;) + geom_line(aes(x=Day_number, y= poly2values),col=&quot;green&quot;)+ geom_line(aes(x=Day_number, y= poly3values),col=&quot;blue&quot;)+ #Add text annotations geom_text(x=10,y=18,label=&quot;data series&quot;,col=&quot;grey&quot;,hjust=0)+ geom_text(x=10,y=16,label=&quot;linear&quot;,col=&quot;red&quot;,hjust=0)+ geom_text(x=10,y=13,label=parse(text=&quot;x^2&quot;),col=&quot;green&quot;,hjust=0)+ geom_text(x=10,y=10,label=parse(text=&quot;x^3&quot;),col=&quot;blue&quot;,hjust=0)+ labs(title = &quot;Estimating Melbourne&#39;s temperature&quot;, subtitle = &#39;Daily maximum temperature recorded in Melbourne in 2019&#39;, caption = &quot;Data: Bureau of Meteorology 2020&quot;) + xlim(0,366)+ ylim(10,45)+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366)) + scale_y_continuous(breaks=c(10,15,20,25,30,35,40,45)) + xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(panel.grid.minor = element_blank()) MEL_weather_model_chart We can see in the chart above the polynomial models do much better at fitting the data. However, they are still highly variant. Just how variant are they? We can look at the residuals to find out. The residuals is the gap between the observed data point (i.e. the grey line) and our model. #Get the residuals for poly1 residuals_poly1 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly1) residuals_poly1_chart &lt;- ggplot(data=residuals_poly1,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;red&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly1_chart #Get the residuals for poly2 residuals_poly2 &lt;- MEL_weather_2019%&gt;% add_residuals(poly2) residuals_poly2_chart &lt;- ggplot(data=residuals_poly2,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;green&quot;, size=1)+ geom_line()+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;)+ theme_bw()+ theme(axis.text=element_text(size=12))+ theme(axis.ticks.x=element_blank(), axis.text.x=element_blank()) residuals_poly2_chart #Get the residuals for poly3 residuals_poly3 &lt;- MEL_weather_2019 %&gt;% add_residuals(poly3) residuals_poly3_chart &lt;- ggplot(data=residuals_poly3,aes(x=Day_number, y=resid))+ geom_ref_line(h=0,colour=&quot;blue&quot;, size=1)+ geom_line()+ theme_bw()+ theme(axis.text=element_text(size=12))+ scale_x_continuous(breaks= c(15,45,75,105,135,165,195,225,255,285,315,345), labels=c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;, &quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;, &quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;, &quot;Dec&quot;), expand=c(0,0), limits=c(0,366))+ xlab(&quot;&quot;)+ ylab(&quot;°C&quot;) residuals_poly3_chart three_charts_single_page &lt;- plot_grid( residuals_poly1_chart, residuals_poly2_chart, residuals_poly3_chart, ncol=1,nrow=3,label_size=16) three_charts_single_page As we move from a linear, to a x², to a x³ model, we see the residuals decrease in volatility. "],["hypothesis-testing.html", "Chapter 5 Hypothesis testing 5.1 A quick refresher 5.2 T-testing our first hypothesis 5.3 Understanding tailed tests 5.4 Correlation 5.5 Confidence intervals for the mean 5.6 Confidence intervals for a model", " Chapter 5 Hypothesis testing 5.1 A quick refresher Hypothesis testing is a way of validating if a claim about a population (e.g. a data set) is correct. Getting data on a whole population (e.g. everyone in Australia) is hard. So, to validate a hypothesis, we use random samples from a population instead. The language when dealing with hypothesis testing is purposefully janky. When looking at the outputs of our hypothesis test, we consider p-values. Note: There’s lots wrong with p-values that we won’t bother getting into right now. The long story short is if you make your null hypothesis ultra specific and only report when your p-value on your millionth iteration of a test is below 0.05… bad science is likely to get published and cited. What we need to know: A small p-value (typically less than or equal to 0.05) indicates strong evidence against the null hypothesis, so we reject it. A large p-value (greater than 0.05) indicates weak evidence against the null hypothesis, so you fail to reject it. Let’s load in some packages and get started. # Load necessary packages library(ggridges) library(ggplot2) library(ggrepel) # Avoid overlapping text in plots library(viridis) # Color scales library(readxl) # Read Excel files library(dplyr) # Data manipulation library(stringr) # String operations library(tidyr) # Data tidying (replaces reshape) library(lubridate) # Work with dates library(gapminder) # Gapminder dataset library(ggalt) # Extensions to ggplot (dumbbell plots, etc.) library(purrr) # Functional programming library(scales) # Scaling tools for ggplot library(aTSA) # Time series analysis library(readrba) # For working with Reserve Bank of Australia data 5.2 T-testing our first hypothesis We’ll start by creating a normally distributed random dataset using rnorm. By default the rnorm function will generate a dataset that has a mean of 0 and a standard deviation of 1, but let’s state it explicitly to keep things simple. set.seed(40) dataset1 &lt;- data.frame(variable1=rnorm(1000,mean=0,sd=1)) Let’s chart the distribution. ggplot() + geom_histogram(aes(x = dataset1$variable1, y = ..density..), binwidth = 0.1, fill = &quot;blue&quot;, alpha = 0.5) + stat_function(fun = dnorm, args = list(mean = mean(dataset1$variable1), sd = sd(dataset1$variable1))) + geom_vline(xintercept = 0, linetype = &quot;dotted&quot;, alpha = 0.5) + labs(title = &quot;Histogram for T-Testing&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme_minimal() + theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.subtitle = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) We know that the mean of dataset1 will be approximately zero (because we set it)… but let’s check anyway. # Find the mean of dataset1 mean(dataset1$variable1) Now let’s run our first hypothesis test. We’ll use the t.test function. This is in the format of t.test(data, null_hypothesis). We’ll start with the null hypothesis that the mean for dataset1$variable1 is 5. This is a two tailed t-test, as we will reject the null if we’re confident the mean is either above or below 5. # Hypothesis test t.test(dataset1$variable1, mu = 5) We see the p-value here is tiny, meaning we reject the null hypothesis. That is to say, the mean for dataset1$variable1 is not 5. 5.3 Understanding tailed tests By default, t.test assumes a two-tailed test with a 95% confidence level. However, sometimes we need to test if one variable is greater or smaller than another (rather than just different from the null). This is when we use one-tailed tests. Next, we test whether the mean is -0.03 using a one-tailed test: # Hypothesis test t.test(dataset1$variable1, mu = -0.03, alternative=&quot;greater&quot;) We see here the p-value is greater than 0.05, leading us to fail to reject the null hypothesis. In a sentence, we cannot say that the mean of dataset1$variable1 is different to 0.01. 5.4 Correlation A correlation coefficient measures the direction and strength of the relationship between two variables. However, correlation calculations assume normal distributions. Let’s examine the relationship between mpg and hp in the mtcars dataset. ggplot2::ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point(color = &quot;blue&quot;, alpha = 0.5) + geom_smooth(method = &#39;lm&#39;) + labs( title = &quot;Horsepower vs. Miles Per Gallon&quot;, x = &quot;Horsepower&quot;, y = &quot;Miles Per Gallon&quot; ) + theme_minimal(base_size = 8) + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11, margin = margin(0, 0, 25, 0)), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), axis.title.y = element_text(margin = margin(r = 3)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -10)), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) We see that miles per gallon is correlated with horsepower. It’s a negative relationship, meaning the more horsepower in a car, the less miles per gallon the car exhibits. We can use the cor.test to tell us the correlation coefficient and the p-value of the correlation. We specify the method as ‘pearson’ for the Pearson correlation coefficient. cor.test(mtcars$hp, mtcars$mpg, method=&quot;pearson&quot;) Since Pearson’s method assumes normality, we check the distributions. Let’s plot a histogram for both hp and mpg. ggplot()+ geom_histogram(aes(x=mtcars$mpg,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot()+ geom_histogram(aes(x=mtcars$hp,y=..density..),fill=&quot;blue&quot;,alpha=0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$hp), sd = sd(mtcars$hp)))+ geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$hp), linetype=&quot;dotted&quot;,alpha=0.5)+ labs(title=&quot;Histogram of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Crikey… they don’t look very normal at all. Let’s plot QQ plots of our variables and see what’s going on. ggplot(mtcars, aes(sample = mpg)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) ggplot(mtcars, aes(sample = hp)) + geom_qq()+ geom_qq_line()+ labs(title=&quot;QQ plot of mtcars$hp&quot;, caption = &quot;Data: Made from mtcars&quot;, x=&quot;&quot;, y=&quot;&quot;) + theme_minimal() + theme(panel.spacing.x = unit(10, &quot;mm&quot;))+ theme(legend.position=&quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=9))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(plot.title = element_text(margin=ggplot2::margin(0,0,15,0))) + theme(plot.margin=unit(c(0.5,0.5,0.5,0.5),&quot;cm&quot;)) Hmm okay, both series are a bit all over the shop. Let’s do a statistical test to confirm. The Shapiro-Wilk’s method is widely used for normality testing. The null hypothesis of this tests is that the sample distribution is normal. If the test is significant, the distribution is non-normal. shapiro.test(mtcars$mpg) shapiro.test(mtcars$hp) 5.5 Confidence intervals for the mean We can calkcualte the confidence interval for the mean. This measures the variance of the normal distribution, and gives us an idea of how ‘clustered’ the values are to the mean. There are 4 steps to do this: Calculate the mean Calculate the standard error of the mean Find the t-score that corresponds to the confidence level Calculate the margin of error and construct the confidence interval mpg.mean &lt;- mean(mtcars$mpg) print(mpg.mean) mpg.n &lt;- length(mtcars$mpg) mpg.sd &lt;- sd(mtcars$mpg) mpg.se &lt;- mpg.sd/sqrt(mpg.n) print(mpg.se) alpha = 0.05 degrees.freedom = mpg.n - 1 t.score = qt(p=alpha/2, df=degrees.freedom,lower.tail=F) print(t.score) mpg.error &lt;- t.score * mpg.se lower.bound &lt;- mpg.mean - mpg.error upper.bound &lt;- mpg.mean + mpg.error print(c(lower.bound,upper.bound)) For the lazy folks among us - there’s also this quick and dirty way of doing it. # Calculate the mean and standard error mpg.model &lt;- lm(mpg ~ 1, mtcars) # Calculate the confidence interval confint(mpg.model, level=0.95) Great. Let’s plot this interval on the distribution. ggplot(mtcars, aes(x = mpg)) + geom_histogram(aes(y = ..density..), binwidth = 2, fill = &quot;blue&quot;, alpha = 0.5) + stat_function(fun = dnorm, args = list(mean = mean(mtcars$mpg), sd = sd(mtcars$mpg))) + geom_hline(yintercept = 0) + geom_vline(xintercept = mean(mtcars$mpg), linetype = &quot;dotted&quot;, alpha = 0.5) + geom_vline(xintercept = c(lower.bound, upper.bound), col = &quot;purple&quot;) + labs( title = &quot;Histogram of mtcars$mpg&quot;, caption = &quot;Data: Made from mtcars&quot;, x = NULL, y = NULL ) + theme_minimal() + theme( panel.spacing.x = unit(10, &quot;mm&quot;), legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12, margin = margin(0, 0, 15, 0)), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 9), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;cm&quot;) ) Two things we note here: Firstly, the distribution doesn’t look that normal. Secondly, the 95% confidence interval looks narrow as a result. Let’s do the same analysis with an actual normal distribution and see what happens. set.seed(404) dataset2 &lt;- data.frame(variable1 = rnorm(1000, mean = 0, sd = 1)) df2.mean &lt;- mean(dataset2$variable1) df2.n &lt;- length(dataset2$variable1) df2.sd &lt;- sd(dataset2$variable1) df2.se &lt;- df2.sd / sqrt(df2.n) # Fixed variable name typo alpha &lt;- 0.05 t.score &lt;- qt(p = alpha / 2, df = df2.n - 1, lower.tail = FALSE) df2.error &lt;- t.score * df2.se lower.bound.df2 &lt;- df2.mean - df2.error upper.bound.df2 &lt;- df2.mean + df2.error # Functions to shade the tails shade_tail &lt;- function(x, bound, direction) { y &lt;- dnorm(x, mean = 0, sd = 1) y[(direction == &quot;upper&quot; &amp; x &lt; bound) | (direction == &quot;lower&quot; &amp; x &gt; bound)] &lt;- NA return(y) } # Plot ggplot(dataset2, aes(x = variable1)) + geom_histogram(aes(y = ..density..), binwidth = 0.1, fill = &quot;blue&quot;, alpha = 0.5) + stat_function(fun = dnorm, args = list(mean = df2.mean, sd = df2.sd)) + geom_hline(yintercept = 0) + geom_vline(xintercept = c(df2.mean, lower.bound.df2, upper.bound.df2), linetype = c(&quot;dotted&quot;, &quot;solid&quot;, &quot;solid&quot;), col = c(&quot;black&quot;, &quot;purple&quot;, &quot;purple&quot;), alpha = 0.5) + stat_function(fun = function(x) shade_tail(x, upper.bound.df2, &quot;upper&quot;), geom = &quot;area&quot;, fill = &quot;grey&quot;, col = &quot;grey&quot;, alpha = 0.8) + stat_function(fun = function(x) shade_tail(x, lower.bound.df2, &quot;lower&quot;), geom = &quot;area&quot;, fill = &quot;grey&quot;, col = &quot;grey&quot;, alpha = 0.8) + labs( title = &quot;95% Confidence Interval&quot;, caption = &quot;Data: Generated from rnorm(1000)&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme_minimal() + scale_x_continuous(breaks = seq(-3, 3, by = 1)) + theme( panel.spacing.x = unit(10, &quot;mm&quot;), legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12, margin = margin(0, 0, 15, 0)), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 9), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), plot.margin = unit(c(0.5, 0.5, 0.5, 0.5), &quot;cm&quot;) ) Great - we’ve got a more sensible looking plot, and greyed out the tails where our confidence interval excludes. We expect out observation to fall somewhere between the two purple lines (or more exactly between -2.1 and 2.1) 5.6 Confidence intervals for a model We can also calculate the confidence interval around a linear model. This process shows how confident we can be about any single point in the linear estimate. If the confidence interval is wide, the estimate at that point is likely unreliable. We’ll create a linear model of mpg on the y-axis and horsepower on the x-axis. mtcars.lm &lt;- lm(mpg ~ hp, data = mtcars) summary(mtcars.lm) predict(mtcars.lm, newdata = mtcars, interval = &#39;confidence&#39;) Thegeom_smooth()function presents an easy way to plot a confidence interval on a chart. The syntax in this example is: geom_smooth(aes(x = hp, y = mpg), method='lm', level=0.95) ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point(col = &quot;blue&quot;, alpha = 0.5) + geom_smooth(method = &quot;lm&quot;, level = 0.95) + labs( title = &quot;Building a Regression Model&quot;, subtitle = &quot;Higher horsepower cars get fewer miles per gallon&quot;, caption = &quot;Data: mtcars dataset&quot;, x = &quot;Horsepower&quot;, y = &quot;Miles per Gallon&quot; ) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11, margin = margin(0, 0, 25, 0)), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.title.y = element_text(margin = margin(r = 3)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -10)), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) For a sanity check, let’s crank up the confidence level to 0.999 (meaning our interval should capture just about all the observations). We see the confidence interval band increases… but not by that much. Why? Well remember how the data isn’t a very good normal distribution? That means the confidence interval function won’t be super accurate - especially at the extremes. "],["forecasting.html", "Chapter 6 Forecasting 6.1 ARIMA models", " Chapter 6 Forecasting So, we’ve got a time series dataset… but what is a reasonable forecast for how it might behave in the future? Sure we can build a confidence interval (as we learned in the previous chapter) and figure out a reasonable value - but what about forecasting for multiple periods into the future? That’s where we need to build models. Let’s load in some packages. # Load in required packages library(tidyverse) # Includes ggplot2, dplyr, purrr, stringr, etc. library(ggridges) # For ridge plots library(forecast) # Time series forecasting library(ggrepel) # For better text label placement library(viridis) # Color scales library(readxl) # Excel file reading library(lubridate) # Date and time manipulation library(gapminder) # Gapminder data for examples library(ggalt) # Additional ggplot2 geoms library(scales) # Scale functions for ggplot2 We’ll start with some pre-loaded time series data. The ggplot2 package includes a data set called ‘economics’ that contains US economic indicators from the 1960’s to 2015. econ_data &lt;- economics %&gt;% dplyr::select(c(&quot;date&quot;, &quot;uempmed&quot;)) econ_data &lt;- econ_data %&gt;% dplyr::filter((date &gt;= as.Date(&quot;1970-01-01&quot;) &amp; date &lt;= as.Date(&quot;1999-12-31&quot;))) As a side note: We can also get Australian unemployment rate data using the readrba function. aus_unemp_rate &lt;- read_rba(series_id = &quot;GLFSURSA&quot;) head(aus_unemp_rate) Let’s plot the US data to see what we are working with. ggplot(econ_data) + geom_point(aes(x = date, y = uempmed), col = &quot;grey&quot;, alpha = 0.5) + geom_smooth(aes(x = date, y = uempmed), col = &quot;blue&quot;) + labs(title = &quot;Unemployment rate&quot;, caption = &quot;Data: ggplot2::economics&quot;, x = &quot;&quot;, y = &quot;&quot;) + theme_minimal() + theme( plot.title = element_text(face = &quot;bold&quot;, size = 12, margin = margin(0, 0, 25, 0)), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), axis.title.y = element_text(margin = margin(t = 0, r = 3, b = 0, l = 0)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -10)), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), legend.position = &quot;bottom&quot; ) 6.1 ARIMA models AutoRegressive Integrated Moving Average (ARIMA) models are a handy tool to have in the toolbox. An ARIMA model describes where Yt depends on its own lags. A moving average (MA only) model is one where Yt depends only on the lagged forecast errors. We combine these together (technically we integrate them) and get ARIMA. When working with ARIMAs, we need to ‘difference’ our series to make it stationary. We check if it is stationary using the augmented Dickey-Fuller test. The null hypothesis assumes that the series is non-stationary. A series is said to be stationary when its mean, variance, and autocovariance don’t change much over time. # Test for stationarity aTSA::adf.test(econ_data$uempmed) # See the auto correlation acf(econ_data$uempmed) # Identify partial auto correlation pacf(econ_data$uempmed) # Take the first differences of the series econ_data &lt;- econ_data %&gt;% mutate(diff = uempmed - lag(uempmed)) # Plot the first differences ggplot(econ_data, aes(x = date, y = diff)) + geom_point(col = &quot;grey&quot;, alpha = 0.5) + geom_smooth(col = &quot;blue&quot;) + labs( title = &quot;1st Difference (Unemployment Rate)&quot;, caption = &quot;Data: ggplot2::economics&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme_minimal() + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12, margin = margin(0, 0, 25, 0)), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), axis.title.y = element_text(margin = margin(t = 0, r = 3, b = 0, l = 0)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -10)), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank() ) # Fit an ARIMA model ARIMA_model &lt;- forecast::auto.arima(econ_data$uempmed) # Display model summary and residual diagnostics summary(ARIMA_model) forecast::checkresiduals(ARIMA_model) # Forecast for the next 36 time periods ARIMA_forecast &lt;- forecast::forecast(ARIMA_model, h = 36, level = c(95)) # Plot the forecast plot(ARIMA_forecast) "],["web-scraping.html", "Chapter 7 Web scraping 7.1 Why it matters 7.2 Anatomy of a webpage 7.3 Scraping a table", " Chapter 7 Web scraping 7.1 Why it matters Collecting data off websites can be a nightmare. The worst case is manually typing data from a web-page into spreadsheets… but there are many steps we can do before resorting to that. This chapter will outline the process for pulling data off the web, and particularly for understanding the exact web-page element we want to extract. The notes and code loosely follow the fabulous data tutorial by Grant R. McDermott in his Data Science for Economists series. It has been updated to scrape the most recent version and structure of the relevant Wikipedia pages. First up, let’s load some packages. # Install development version of rvest if necessary if (numeric_version(packageVersion(&quot;rvest&quot;)) &lt; numeric_version(&#39;0.99.0&#39;)) { remotes::install_github(&#39;tidyverse/rvest&#39;) } # Load and install the packages that we&#39;ll be using today if (!require(&quot;pacman&quot;)) install.packages(&quot;pacman&quot;) pacman::p_load(tidyverse, rvest, lubridate, janitor, data.table, hrbrthemes) library(ggplot2) library(dplyr) library(tidyverse) 7.2 Anatomy of a webpage Web pages can be categorized as either server-side rendered (where content is embedded in the HTML) or client-side rendered (where content loads dynamically using JavaScript). When scraping server-side rendered pages, locating the correct CSS or XPath selectors is crucial. Trawling through CSS code on a webpage is a bit of a nightmare - so we’ll use a chrome extension called SelectGadget to help. The R package that’s going to do the heavy lifting is called rvest and is based on the python package called Beauty Soup. 7.3 Scraping a table Let’s use this wikipedia page as a starting example. It contains various entries for the men’s 100m running record. We can start by pulling all the data from the webpage. m100 &lt;- rvest:: read_html( &quot;http://en.wikipedia.org/wiki/Men%27s_100_metres_world_record_progression&quot;) m100 ## {html_document} ## &lt;html class=&quot;client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available&quot; lang=&quot;en&quot; dir=&quot;ltr&quot;&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body class=&quot;skin--responsive skin-vector skin-vector-search-vue mediawik ... …and we get a whole heap of mumbo jumbo. To get the table of ‘Unofficial progression before the IAAF’ we’re going to have to be more specific. Using the SelectGadget tool we can click around and identify that that specific table. pre_iaaf = m100 %&gt;% html_element(&quot;div+ .wikitable :nth-child(1)&quot;) %&gt;% ## select table element html_table() ## convert to data frame pre_iaaf ## # A tibble: 21 × 5 ## Time Athlete Nationality `Location of races` Date ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 10.8 Luther Cary United States Paris, France July 4, 1… ## 2 10.8 Cecil Lee United Kingdom Brussels, Belgium September… ## 3 10.8 Étienne De Ré Belgium Brussels, Belgium August 4,… ## 4 10.8 L. Atcherley United Kingdom Frankfurt/Main, Germany April 13,… ## 5 10.8 Harry Beaton United Kingdom Rotterdam, Netherlands August 28… ## 6 10.8 Harald Anderson-Arbin Sweden Helsingborg, Sweden August 9,… ## 7 10.8 Isaac Westergren Sweden Gävle, Sweden September… ## 8 10.8 Isaac Westergren Sweden Gävle, Sweden September… ## 9 10.8 Frank Jarvis United States Paris, France July 14, … ## 10 10.8 Walter Tewksbury United States Paris, France July 14, … ## # ℹ 11 more rows Niiiiice - now that’s better. Let’s do some quick data cleaning. pre_iaaf &lt;- pre_iaaf %&gt;% clean_names() %&gt;% mutate(date = mdy(date)) pre_iaaf ## # A tibble: 21 × 5 ## time athlete nationality location_of_races date ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 10.8 Luther Cary United States Paris, France 1891-07-04 ## 2 10.8 Cecil Lee United Kingdom Brussels, Belgium 1892-09-25 ## 3 10.8 Étienne De Ré Belgium Brussels, Belgium 1893-08-04 ## 4 10.8 L. Atcherley United Kingdom Frankfurt/Main, Germany 1895-04-13 ## 5 10.8 Harry Beaton United Kingdom Rotterdam, Netherlands 1895-08-28 ## 6 10.8 Harald Anderson-Arbin Sweden Helsingborg, Sweden 1896-08-09 ## 7 10.8 Isaac Westergren Sweden Gävle, Sweden 1898-09-11 ## 8 10.8 Isaac Westergren Sweden Gävle, Sweden 1899-09-10 ## 9 10.8 Frank Jarvis United States Paris, France 1900-07-14 ## 10 10.8 Walter Tewksbury United States Paris, France 1900-07-14 ## # ℹ 11 more rows Let’s also scrape the data for the more recent running records. That’s the tables named ‘Records (1912-1976)’ and ‘Records since 1977’. For the second table: iaaf_76 = m100 %&gt;% html_element(&quot;#mw-content-text &gt; div &gt; table:nth-child(17)&quot;) %&gt;% html_table() iaaf_76 &lt;-iaaf_76 %&gt;% clean_names() %&gt;% mutate(date = mdy(date)) iaaf_76 ## # A tibble: 54 × 8 ## time wind auto athlete nationality location_of_race date ref ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 10.6 &quot;&quot; NA Donald Lippi… United Sta… Stockholm, Swed… 1912-07-06 [2] ## 2 10.6 &quot;&quot; NA Jackson Scho… United Sta… Stockholm, Swed… 1920-09-16 [2] ## 3 10.4 &quot;&quot; NA Charley Padd… United Sta… Redlands, USA 1921-04-23 [2] ## 4 10.4 &quot;0.0&quot; NA Eddie Tolan United Sta… Stockholm, Swed… 1929-08-08 [2] ## 5 10.4 &quot;&quot; NA Eddie Tolan United Sta… Copenhagen, Den… 1929-08-25 [2] ## 6 10.3 &quot;&quot; NA Percy Willia… Canada Toronto, Canada 1930-08-09 [2] ## 7 10.3 &quot;0.4&quot; 10.4 Eddie Tolan United Sta… Los Angeles, USA 1932-08-01 [2] ## 8 10.3 &quot;&quot; NA Ralph Metcal… United Sta… Budapest, Hunga… 1933-08-12 [2] ## 9 10.3 &quot;&quot; NA Eulace Peaco… United Sta… Oslo, Norway 1934-08-06 [2] ## 10 10.3 &quot;&quot; NA Chris Berger Netherlands Amsterdam, Neth… 1934-08-26 [2] ## # ℹ 44 more rows And now for the third table: iaaf &lt;- m100 %&gt;% html_element(&quot;#mw-content-text &gt; div.mw-parser-output &gt; table:nth-child(23)&quot;) %&gt;% html_table() %&gt;% clean_names() %&gt;% mutate(date = mdy(date)) iaaf ## # A tibble: 24 × 9 ## time wind auto athlete nationality location_of_race date ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 10.1 1.3 NA Bob Hayes United States Tokyo, Japan 1964-10-15 ## 2 10.0 0.8 NA Jim Hines United States Sacramento, USA 1968-06-20 ## 3 10.0 2.0 NA Charles Greene United States Mexico City, Mexico 1968-10-13 ## 4 9.95 0.3 NA Jim Hines United States Mexico City, Mexico 1968-10-14 ## 5 9.93 1.4 NA Calvin Smith United States Colorado Springs, … 1983-07-03 ## 6 9.83 1.0 NA Ben Johnson Canada Rome, Italy 1987-08-30 ## 7 9.93 1.0 NA Carl Lewis United States Rome, Italy 1987-08-30 ## 8 9.93 1.1 NA Carl Lewis United States Zürich, Switzerland 1988-08-17 ## 9 9.79 1.1 NA Ben Johnson Canada Seoul, South Korea 1988-09-24 ## 10 9.92 1.1 NA Carl Lewis United States Seoul, South Korea 1988-09-24 ## # ℹ 14 more rows ## # ℹ 2 more variables: notes_note_2 &lt;chr&gt;, duration_of_record &lt;chr&gt; How good. Now let’s bind the rows together to make a master data set. wr100 &lt;- rbind( pre_iaaf %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;% mutate(era = &quot;Pre-IAAF&quot;), iaaf_76 %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;% mutate(era = &quot;Pre-automatic&quot;), iaaf %&gt;% dplyr::select(time, athlete, nationality, date) %&gt;% mutate(era = &quot;Modern&quot;) ) wr100 ## # A tibble: 99 × 5 ## time athlete nationality date era ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; ## 1 10.8 Luther Cary United States 1891-07-04 Pre-IAAF ## 2 10.8 Cecil Lee United Kingdom 1892-09-25 Pre-IAAF ## 3 10.8 Étienne De Ré Belgium 1893-08-04 Pre-IAAF ## 4 10.8 L. Atcherley United Kingdom 1895-04-13 Pre-IAAF ## 5 10.8 Harry Beaton United Kingdom 1895-08-28 Pre-IAAF ## 6 10.8 Harald Anderson-Arbin Sweden 1896-08-09 Pre-IAAF ## 7 10.8 Isaac Westergren Sweden 1898-09-11 Pre-IAAF ## 8 10.8 Isaac Westergren Sweden 1899-09-10 Pre-IAAF ## 9 10.8 Frank Jarvis United States 1900-07-14 Pre-IAAF ## 10 10.8 Walter Tewksbury United States 1900-07-14 Pre-IAAF ## # ℹ 89 more rows Excellent. Let’s plot the results. ggplot(wr100) + geom_point(aes(x = date, y = time, col = era), alpha = 0.7) + labs( title = &quot;Men&#39;s 100m World Record Progression&quot;, subtitle = &quot;Analysing how times have improved over the past 130 years&quot;, caption = &quot;Data: Wikipedia 2025&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme_minimal() + scale_y_continuous(limits = c(9.5, 11), breaks = c(9.5, 10, 10.5, 11)) + theme( axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -20)), plot.subtitle = element_text(margin = margin(0, 0, 25, 0),size=11), legend.title = element_blank(), plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) "],["text-mining.html", "Chapter 8 Text mining 8.1 Frequency analysis 8.2 Sentiment analysis", " Chapter 8 Text mining Numbers are great… but words literally tell a story. Analysing text (e.g. books, tweets, survey responses) in a quantitative format is naturally challenging - however there’s a few tricks which can simplify the process. This chapter outlines the process for inputting text data, and running some simple analysis. The notes and code loosely follow the fabulous book Text Mining with R by Julia Silge and David Robinson. First up, let’s load some packages. library(ggplot2) library(dplyr) library(tidyverse) library(tidytext) library(textdata) 8.1 Frequency analysis There’s a online depository called Project Gutenberg which catalogue texts that have lost their copyright. It just so happens that The Bible is on this list. Let’s check out the most frequent words. library(tidyverse) library(tidytext) # Correct URL for the raw text file bible_url &lt;- &quot;https://raw.githubusercontent.com/charlescoverdale/casualdabbler2e/main/data/bible.txt&quot; # Read the text file directly from the URL bible &lt;- read_lines(bible_url) # Convert to a tibble bible_df &lt;- tibble(text = bible) # Tokenize words and remove stop words bible_tidy &lt;- bible_df %&gt;% unnest_tokens(word, text) %&gt;% anti_join(stop_words, by = &quot;word&quot;) # Find and display the most common words common_words &lt;- bible_tidy %&gt;% count(word, sort = TRUE) %&gt;% head(20) # Show the top 20 words print(common_words) ## # A tibble: 20 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 lord 7830 ## 2 thou 5474 ## 3 thy 4600 ## 4 god 4446 ## 5 ye 3983 ## 6 thee 3827 ## 7 1 2830 ## 8 2 2724 ## 9 3 2570 ## 10 israel 2565 ## 11 4 2476 ## 12 son 2370 ## 13 7 2351 ## 14 5 2308 ## 15 6 2297 ## 16 hath 2264 ## 17 king 2264 ## 18 9 2210 ## 19 8 2193 ## 20 people 2142 Somewhat unsurprisingly - “lord” wins it by a country mile. 8.2 Sentiment analysis Just like a frequency analysis, we can do a ‘vibe’ analysis (i.e. sentiment of a text) using a clever thesaurus matching technique. In the tidytext package are lexicons which include the general sentiment of words (e.g. the emotion you can use to describe that word). Let’s see the count of words most associated with ‘joy’ in the bible. # Tokenize words bible_tidy &lt;- bible_df %&gt;% unnest_tokens(word, text) %&gt;% mutate(word = tolower(word)) # Ensure lowercase matching # Get NRC lexicon &amp; filter for &quot;joy&quot; nrcjoy &lt;- tidytext::get_sentiments(&quot;nrc&quot;) %&gt;% filter(sentiment == &quot;joy&quot;) # Join words with NRC joy sentiment list &amp; count occurrences bible_joy_words &lt;- bible_tidy %&gt;% inner_join(nrcjoy, by = &quot;word&quot;) %&gt;% count(word, sort = TRUE) # View top joyful words print(bible_joy_words) ## # A tibble: 264 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 god 4446 ## 2 good 720 ## 3 art 494 ## 4 peace 429 ## 5 found 404 ## 6 glory 402 ## 7 daughter 324 ## 8 pray 313 ## 9 love 310 ## 10 blessed 302 ## # ℹ 254 more rows "],["geocoding.html", "Chapter 9 Geocoding", " Chapter 9 Geocoding "],["drivetime-analysis.html", "Chapter 10 Drivetime analysis", " Chapter 10 Drivetime analysis "],["raster-data.html", "Chapter 11 Raster data 11.1 Import raster 11.2 Choosing geographies 11.3 Analysis with raster data 11.4 Making an interactive map", " Chapter 11 Raster data Raster data (sometimes referred to as gridded data) is a type of spatial data that is stored in a grid rather than a polygon. Imagine a chessboard of individuals squares covering the Australian landmass compared to 8 different shapes covering each state and territory. The nice thing about gridded data is that all the cells in the grid are the same size - making calculations much easier. The tricky part is those squares are overlayed on a (very roughly) spherical earth. We’ll have to think about mapping projections along the way. Let’s get started. First up we need to load some spatial and data crunching packages. # Core tidyverse packages (includes ggplot2, dplyr, tidyr, broom, etc.) library(tidyverse) # Spatial data handling library(sf) # Modern replacement for sp + rgdal library(raster) # Working with raster data library(ncdf4) # NetCDF file handling library(leaflet) # Interactive maps # Mapping &amp; Visualization library(ggspatial) # Adds scale bars, north arrows, etc. #library(tmap) # Thematic mapping library(ggmap) # Google Maps integration library(rasterVis) # Raster visualization library(viridis) # Color scales for ggplot2 and rasterVis library(RColorBrewer) # Color palettes # Other Utilities library(absmapsdata) # Australian Bureau of Statistics map data 11.1 Import raster Next, we want to import annual rainfall data from github (original source available from the Bureau of Meterology) rainfall &lt;- raster(&quot;https://raw.github.com/charlescoverdale/ERF/master/rainan.txt&quot;) plot(rainfall) Straight away we see this is for the whole of Australia (and then some). We’re only interested in what’s going on in QLD… so let’s crop the data down to scale. For this we’ll need to import a shapefile for QLD. The easiest way to do this is using the absmapsdata package - importing a shapefile of Australia then filtering for only Queensland. # Import a polygon for the state of Queensland QLD_shape &lt;- absmapsdata::state2016 %&gt;% filter(state_name_2016==&quot;Queensland&quot;) 11.2 Choosing geographies We have raster data for the entirety of Australia (and then some as it’s pulled from one of the BOMs satellites). This is a bit messy to work with - so let’s crop the rainfall data from the entire Australian continent to just Queensland. #Crop data r2 &lt;- crop(rainfall,extent(QLD_shape)) r3 &lt;- mask(r2,QLD_shape) plot(r3) ]Next up, let’s transform the cropped raster into a data frame that we can use in the ggplot package. r3_df &lt;- as.data.frame(r3,xy=TRUE) r3_df &lt;- r3_df %&gt;% filter(rainan!=&quot;NA&quot;) ggplot() + geom_tile(data=r3_df, aes(x=x, y=y, fill=rainan)) + scale_fill_viridis() + coord_equal() + theme(legend.position=&quot;bottom&quot;) + theme(legend.key.width=unit(1.2, &quot;cm&quot;))+ labs(title=&quot;Rainfall in QLD&quot;, subtitle = &quot;Analysis from the Bureau of Meterology&quot;, caption = &quot;Data: BOM 2021&quot;, x=&quot;&quot;, y=&quot;&quot;, fill=&quot;(mm)&quot;) + theme_minimal() + theme(axis.ticks.x = element_blank(),axis.text.x = element_blank())+ theme(axis.ticks.y = element_blank(),axis.text.y = element_blank())+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ #theme(legend.position = &quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) Excellent, we’ve got a working map of rainfall in Queensland using the ggplot package. We’ll tidy up the map also and add a title and some better colours. 11.3 Analysis with raster data For our first piece of data analysis, we’re going to look at areas with less than 600mm of annual rainfall. How many of our data points will have less than 600mm of rain? Let’s take a look at the data distribution and find out. ggplot(r3_df) + geom_histogram(aes(rainan),binwidth=1,col=&quot;darkblue&quot;)+ labs(title=&quot;Distribution of annual rainfall in QLD&quot;, subtitle = &quot;Data using a 5x5km grid&quot;, caption = &quot;Data: Bureau of Meterology 2021&quot;, x=&quot;Rainfall (mm)&quot;, y=&quot;&quot;, fill=&quot;(mm)&quot;) + theme_minimal() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+ theme(legend.position = &quot;none&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;,size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8)) Interesting. The data is heavily right tailed skewed (the mean will be much higher than the median)… and most of the data looks to be between 0-1000mm (this makes sense). Let’s create a ‘flag’ column of 0’s and 1’s that shows when a data point is less than 600mm. # Define flag column and colors r3_df &lt;- r3_df %&gt;% mutate(flag_600mm = as.factor(ifelse(rainan &lt;= 600, 1, 0))) flag_colours &lt;- c(&quot;grey&quot;, &quot;#2FB300&quot;) # Plot ggplot(r3_df, aes(x = x, y = y, fill = flag_600mm)) + geom_tile() + scale_fill_manual(values = flag_colours) + coord_equal() + theme_minimal() + labs( title = &quot;Areas with Less than 600mm of Annual Rainfall in QLD&quot;, subtitle = &quot;Identifying Suitable Land Parcels for ERF Plantings&quot;, caption = &quot;Data: Bureau of Meteorology 2021&quot;, x = &quot;&quot;, y = &quot;&quot;, fill = &quot;(mm)&quot; ) + theme( axis.ticks = element_blank(), axis.text = element_blank(), panel.grid = element_blank(), legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8) ) 11.4 Making an interactive map The static map above is good… but an interactive visual (ideally with place names below) is better. The leaflet package is exceptional here. # This code creates an interactive leaflet map. It is commented out to allow the html version of RCCD2e to render online. # leaflet() %&gt;% # addTiles() %&gt;% # addRasterImage(r5, opacity = 0.5) %&gt;% # addLegend(pal = colorNumeric(&quot;viridis&quot;, values(r5)), values = values(r5)) "],["australian-economic-data.html", "Chapter 12 Australian economic data 12.1 GDP 12.2 Unemployment rate 12.3 Inflation (CPI) 12.4 Wage Price Index 12.5 AUD exchange rate", " Chapter 12 Australian economic data Australia has exceptional financial and economic institutions. Three of these institutions release periodic data useful for economic analysis: Australian Bureau of Statistics Reserve Bank of Australia Treasury As usual, there are catches. Most of this data is in inconsistent formats (the reasons for which continue to baffle me). What’s more, it’s currently not possible to ping databases or API’s for access to this data… it is mainly accessed through spreadsheets. The scripts below run through some of the main ways to import, clean, and analyse Australian macroeconomic data in R. Some of the key packages we’ll use are readabs and readrba. To get started, let’s install and load packages. # Loads the required packages pacman::p_load( ggmap, tmaptools, RCurl, jsonlite, tidyverse, leaflet, writexl, readr, readxl, readabs, readrba, lubridate, zoo, scales ) ## ## There are binary versions available but the source versions are later: ## binary source needs_compilation ## sf 1.0-15 1.0-19 TRUE ## stars 0.5-5 0.6-8 FALSE ## tmaptools 3.1-1 3.2 FALSE 12.1 GDP To get GDP data from the ABS, we’ll use the read_abs function from the readrba package. #For simplicity, we keep the download function seperate to the analysis all_gdp &lt;- read_abs(&quot;5206.0&quot;) #Select the seasonally adjusted data and filter for data and value columns gdp_level &lt;- all_gdp %&gt;% filter(series == &quot;Gross domestic product: Chain volume measures ;&quot;, !is.na(value)) %&gt;% filter(series_type ==&quot;Seasonally Adjusted&quot;) %&gt;% dplyr::select(date,value) %&gt;% dplyr::rename(quarterly_output=value) gdp_level &lt;- gdp_level %&gt;% mutate(quarterly_growth_rate = ((quarterly_output / lag(quarterly_output,1)-1))*100) %&gt;% mutate(annual_gdp = rollapply(quarterly_output, 4, sum, na.rm=TRUE, fill = NA, align = &quot;right&quot;)) %&gt;% mutate(annual_gdp_trillions=annual_gdp/1000000)%&gt;% mutate(annual_growth_rate = ((annual_gdp / lag(annual_gdp, 4) - 1))*100)%&gt;% mutate(Quarter_of_year = lubridate::quarter(date, with_year = FALSE, fiscal_start = 1)) #Set a baseline value gdp_level$baseline_value &lt;- gdp_level$quarterly_output[ which(gdp_level$date ==&quot;2022-03-01&quot;)] gdp_level &lt;- gdp_level %&gt;% mutate(baseline_comparison = (quarterly_output/baseline_value)*100) tail(gdp_level) ## # A tibble: 6 × 9 ## date quarterly_output quarterly_growth_rate annual_gdp ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-06-01 646331 0.334 2567511 ## 2 2023-09-01 649432 0.480 2581019 ## 3 2023-12-01 650532 0.169 2590477 ## 4 2024-03-01 651554 0.157 2597849 ## 5 2024-06-01 652734 0.181 2604252 ## 6 2024-09-01 654912 0.334 2609732 ## # ℹ 5 more variables: annual_gdp_trillions &lt;dbl&gt;, annual_growth_rate &lt;dbl&gt;, ## # Quarter_of_year &lt;int&gt;, baseline_value &lt;dbl&gt;, baseline_comparison &lt;dbl&gt; Now we can plot the GDP data for Australia. plot_gdp &lt;- ggplot(data=gdp_level)+ geom_line((aes(x=date, y=annual_gdp_trillions)), col=&quot;blue&quot;) + labs(title = &quot;Australian GDP ($AUD)&quot;, subtitle = &quot;Annualised figures&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;&quot;, x = &quot; &quot;)+ scale_y_continuous(breaks = c(0,0.5,1.0,1.5,2.0,2.5), labels = label_number(suffix = &quot; trillion&quot;))+ scale_x_date(date_breaks = &quot;10 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -45)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_gdp 12.2 Unemployment rate Download the data #Download the time sreies all_unemployment &lt;- read_abs(&quot;6202.0&quot;) Clean and analyse the data unemployment_rate &lt;- all_unemployment %&gt;% filter(series == &quot;Unemployment rate ; Persons ;&quot;,!is.na(value)) %&gt;% filter(table_title==&quot;Table 1. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original&quot;) %&gt;% filter(series_type ==&quot;Seasonally Adjusted&quot;) %&gt;% mutate(mean_unemployment_rate=mean(value)) %&gt;% mutate(percentile_25=quantile(value,0.25))%&gt;% mutate(percentile_75=quantile(value,0.75)) %&gt;% dplyr::select(date,value,mean_unemployment_rate,percentile_25,percentile_75) tail(unemployment_rate) ## # A tibble: 6 × 5 ## date value mean_unemployment_rate percentile_25 percentile_75 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2024-07-01 4.22 6.58 5.27 7.92 ## 2 2024-08-01 4.13 6.58 5.27 7.92 ## 3 2024-09-01 4.07 6.58 5.27 7.92 ## 4 2024-10-01 4.11 6.58 5.27 7.92 ## 5 2024-11-01 3.93 6.58 5.27 7.92 ## 6 2024-12-01 3.98 6.58 5.27 7.92 Plot the data plot_unemployment_rate &lt;- ggplot(data=unemployment_rate)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;Unemployment rate&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;Unemployment rate (%)&quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;10 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_unemployment_rate 12.3 Inflation (CPI) Download the data all_CPI &lt;- read_abs(&quot;6401.0&quot;) Clean and analyse the data Australia_CPI &lt;- all_CPI %&gt;% filter(series == &quot;Percentage Change from Corresponding Quarter of Previous Year ; All groups CPI ; Australia ;&quot;,!is.na(value)) %&gt;% mutate(mean_CPI=mean(value)) %&gt;% mutate(percentile_25=quantile(value,0.25))%&gt;% mutate(percentile_75=quantile(value,0.75)) %&gt;% dplyr::select(date, value,mean_CPI,percentile_25,percentile_75) tail(Australia_CPI) ## # A tibble: 6 × 5 ## date value mean_CPI percentile_25 percentile_75 ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-09-01 5.4 4.95 1.9 7.3 ## 2 2023-12-01 4.1 4.95 1.9 7.3 ## 3 2024-03-01 3.6 4.95 1.9 7.3 ## 4 2024-06-01 3.8 4.95 1.9 7.3 ## 5 2024-09-01 2.8 4.95 1.9 7.3 ## 6 2024-12-01 2.4 4.95 1.9 7.3 #Can add in the below line to filter #filter(date&gt;&quot;2010-01-01&quot;) %&gt;% Plot the data plot_CPI &lt;- ggplot(data = Australia_CPI %&gt;% filter(date &gt; as.Date(&quot;2000-01-01&quot;))) + geom_rect(aes(xmin = as.Date(&quot;2000-01-01&quot;), xmax = as.Date(&quot;2025-03-01&quot;), ymin = 2, ymax = 3), alpha = 0.1, # Adjusted alpha for better visibility fill = &quot;lightgrey&quot;) + geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + scale_x_date(date_breaks = &quot;2 years&quot;, date_labels = &quot;%Y&quot;) + labs(title = &quot;Inflation (as measured by the CPI)&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot;&quot;) + scale_y_continuous(labels = scales::comma) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + theme(plot.title = element_text(face = &quot;bold&quot;, size = 12)) + theme(plot.subtitle = element_text(size = 11)) + theme(plot.caption = element_text(size = 8)) + theme(axis.text = element_text(size = 8)) + theme(panel.grid.minor = element_blank()) + theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0))) + theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15))) + theme(axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4)) + theme(axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4)) plot_CPI Plot a histogram of the data plot_CPI_hist &lt;- ggplot(Australia_CPI, aes(x = value)) + geom_histogram(aes(y = ..density..), colour = &quot;black&quot;, fill = &quot;lightblue&quot;) + geom_density(alpha = .5, fill = &quot;grey&quot;, colour = &quot;darkblue&quot;) + scale_x_continuous(expand = c(0, 0)) + # Remove extra space on the x-axis labs(title = &quot;Consumer Price Index: Histogram&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot;&quot;) + scale_y_continuous(labels = scales::percent, expand = c(0, 0)) + # Ensure no space on y-axis theme_minimal() + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11, margin = margin(b = 15)), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 20, l = 0)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -2)), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4) ) plot_CPI_hist 12.4 Wage Price Index Download the data all_wpi &lt;- read_abs(&quot;6345.0&quot;) Clean and analyse the data Australia_WPI &lt;- all_wpi %&gt;% filter(series == &quot;Percentage Change From Corresponding Quarter of Previous Year ; Australia ; Total hourly rates of pay excluding bonuses ; Private and Public ; All industries ;&quot;, !is.na(value)) %&gt;% filter(series_type==&quot;Seasonally Adjusted&quot;) %&gt;% mutate(mean_WPI=mean(value)) %&gt;% dplyr::select(date, value,mean_WPI) tail(Australia_WPI) ## # A tibble: 6 × 3 ## date value mean_WPI ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2023-06-01 3.6 3.12 ## 2 2023-09-01 4 3.12 ## 3 2023-12-01 4.3 3.12 ## 4 2024-03-01 4.1 3.12 ## 5 2024-06-01 4.1 3.12 ## 6 2024-09-01 3.5 3.12 Plot the data plot_WPI &lt;- ggplot(data=Australia_WPI)+ geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs(title = &quot;Wage Price Index&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Australian Bureau of Statistics&quot;, y = &quot;(%)&quot;, x = &quot; &quot;)+ scale_y_continuous(labels = scales::comma)+ scale_x_date(date_breaks = &quot;5 years&quot;, date_labels=&quot;%Y&quot;)+ theme_minimal() + theme(legend.position=&quot;bottom&quot;)+ theme(plot.title=element_text(face=&quot;bold&quot;, size=12))+ theme(plot.subtitle=element_text(size=11))+ theme(plot.caption=element_text(size=8))+ theme(axis.text=element_text(size=8))+ theme(panel.grid.minor = element_blank())+ theme(panel.grid.major.x = element_blank()) + theme(axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)))+ theme(axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)))+ theme(axis.line.x = element_line(colour =&quot;black&quot;, size=0.4))+ theme(axis.ticks.x = element_line(colour =&quot;black&quot;, size=0.4)) plot_WPI 12.5 AUD exchange rate God knows why - but there are super quirky names for the official exchange rate tables Download the data exchange_rate_all&lt;- readrba::read_rba(table_no = (c(&quot;ex_daily_8386&quot;, &quot;ex_daily_8790&quot;, &quot;ex_daily_9194&quot;, &quot;ex_daily_9598&quot;, &quot;ex_daily_9902&quot;, &quot;ex_daily_0306&quot;, &quot;ex_daily_0709&quot;, &quot;ex_daily_1013&quot;, &quot;ex_daily_1417&quot;, &quot;ex_daily_18cur&quot;)), cur_hist = &quot;historical&quot;) Clean and analyse the data exchange_rate_AUD &lt;- exchange_rate_all %&gt;% filter(series==&quot;A$1=USD&quot;) %&gt;% dplyr::select(date, value) tail(exchange_rate_AUD) ## # A tibble: 6 × 2 ## date value ## &lt;date&gt; &lt;dbl&gt; ## 1 2017-12-20 0.766 ## 2 2017-12-21 0.767 ## 3 2017-12-22 0.771 ## 4 2017-12-27 0.774 ## 5 2017-12-28 0.779 ## 6 2017-12-29 0.78 Plot the data plot_exchange_rate_AUD &lt;- ggplot(data = exchange_rate_AUD) + geom_line(aes(x = date, y = value), col = &quot;blue&quot;) + labs( title = &quot;AUD Exchange Rate&quot;, subtitle = &quot;Subtitle goes here&quot;, caption = &quot;Data: Reserve Bank of Australia&quot;, y = &quot;Exchange rate&quot;, # Adding a Y-axis label x = &quot;Date&quot; ) + scale_y_continuous(labels = scales::comma) + scale_x_date(date_breaks = &quot;3 years&quot;, date_labels = &quot;%Y&quot;) + theme_minimal() + theme( plot.title = element_text(face = &quot;bold&quot;, size = 12), plot.subtitle = element_text(size = 11), plot.caption = element_text(size = 8), axis.text = element_text(size = 8), axis.title.y = element_text(margin = margin(t = 0, r = 0, b = 0, l = 0)), axis.text.y = element_text(vjust = -0.5, margin = margin(l = 20, r = -15)), axis.line.x = element_line(colour = &quot;black&quot;, size = 0.4), axis.ticks.x = element_line(colour = &quot;black&quot;, size = 0.4), panel.grid.minor = element_blank(), panel.grid.major.x = element_blank(), legend.position = &quot;bottom&quot; ) plot_exchange_rate_AUD "],["australian-election-data.html", "Chapter 13 Australian election data", " Chapter 13 Australian election data "],["bayesian-for-the-common-man.html", "Chapter 14 Bayesian for the common man", " Chapter 14 Bayesian for the common man "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
